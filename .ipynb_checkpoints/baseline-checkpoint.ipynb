{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "w2zS1z71y653"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SW Challenge 2024\n",
        "\n",
        "이 문서는 SW Challenge 2024 zero-shot classification 코드를 포함함. 해당의 challenge의 목표는 multi-modal (vision, language) model 을 이용한 zero-shot classification task 의 성능을 높이는 것 임. 아래 예시 코드를 돌리면 baseline 모델 (CLIP, ViT-B/16) 을 활용해서 Scene dataset 에서 zero-shot classification 결과를 csv 파일 형식으로 저장함. 해당 코드를 참고해서 zero-shot classification 성능을 향상하는 것이 해당 과제의 목표임.\n",
        "\n",
        "### 1. 문제 설명\n",
        "Zero-shot classification 이란 한 번도 학습하지 않은 class 에 대해서 모델이 language 정보를 활용해서 예측하는 것을 의미함. Language 정보를 활용하기 위해서 multi-modal (vision, language) model 을 사용함. 이미지가 주어졌을 때 multi-modal model 은 하나의 이미지 임베딩, 전체 클래스 개수만큼의 텍스트 임베딩을 추출함. 그리고 이미지 임베딩과 텍스트 임베딩간의 거리를 계산한 뒤 가장 거리가 가까운 텍스트 임베딩의 원본 클래스를 정답으로 예측함.\n",
        "\n",
        "### 2. 데이터셋 설명\n",
        "Scene dataset statistics:\n",
        "- #Samples: 8100\n",
        "- #Classes: 6\n",
        "\n",
        "### 3. 규칙\n",
        "- Model 을 제공하는 데이터셋에 직접 학습하는것은 규칙에 위배됨.\n",
        "- 더 자세한 규칙은 kaggle competition homepage 에 규칙 탭을 참고 바람."
      ],
      "metadata": {
        "id": "TRS9-1Ix9V9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 데이터셋 및 라이브러리 설치"
      ],
      "metadata": {
        "id": "w2zS1z71y653"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc -O 2024swunivchallenge.zip https://huggingface.co/datasets/sw24/sw24/resolve/main/data.zip?download=true && unzip -q -n 2024swunivchallenge.zip\n",
        "!pip3 -q install open_clip_torch natsort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgmVR8Ney5sF",
        "outputId": "1816d084-fcbf-48b9-d3cf-1ef046a74465"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-29 11:23:09--  https://huggingface.co/datasets/sw24/sw24/resolve/main/data.zip?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 18.67.181.126, 18.67.181.36, 18.67.181.100, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.67.181.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/88/4b/884bf409ac92c279aafc40bc0f36a43146f1d6f7b6ac4c45040d3f43907e9ec9/7bfbb28b490da1d1b5123758da505495fd3c23436a9a060fa9cb1471d99e7ce6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27data.zip%3B+filename%3D%22data.zip%22%3B&response-content-type=application%2Fzip&Expires=1722511389&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjUxMTM4OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzg4LzRiLzg4NGJmNDA5YWM5MmMyNzlhYWZjNDBiYzBmMzZhNDMxNDZmMWQ2ZjdiNmFjNGM0NTA0MGQzZjQzOTA3ZTllYzkvN2JmYmIyOGI0OTBkYTFkMWI1MTIzNzU4ZGE1MDU0OTVmZDNjMjM0MzZhOWEwNjBmYTljYjE0NzFkOTllN2NlNj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=MEXlZXl1OnYfmkYhT-dQZfLDJh5O7gSSJflwsSHdm9XmxmbaMX-dpB2RevUoJJ5Crn6ZGRpBb3H6yWhnuV8yaHpaDf9H1sJRQhRX56wAxm9Re5WDsFuJR3A3MdS9ipYg%7EuywQ55WoUagR%7Ee8W%7EZziC5oG7wMx8PRheYHJSq%7EMMWlTwsMPXLNCuW%7EGZfPQvZb%7EuoHPlCaYU0o24Iut31cKbCDOrW2wIvy8FY6rwRpygLLNiFmirfc-aWKWVTadnJIGZNL73Fbz8yfatdJcmVgAf9-wud3qnYVxievyq8bdbL37FyxsIlfRXlFl0dXmW5o-a9JX2BeMyR2ALMKoASpqA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2024-07-29 11:23:09--  https://cdn-lfs-us-1.huggingface.co/repos/88/4b/884bf409ac92c279aafc40bc0f36a43146f1d6f7b6ac4c45040d3f43907e9ec9/7bfbb28b490da1d1b5123758da505495fd3c23436a9a060fa9cb1471d99e7ce6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27data.zip%3B+filename%3D%22data.zip%22%3B&response-content-type=application%2Fzip&Expires=1722511389&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjUxMTM4OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzg4LzRiLzg4NGJmNDA5YWM5MmMyNzlhYWZjNDBiYzBmMzZhNDMxNDZmMWQ2ZjdiNmFjNGM0NTA0MGQzZjQzOTA3ZTllYzkvN2JmYmIyOGI0OTBkYTFkMWI1MTIzNzU4ZGE1MDU0OTVmZDNjMjM0MzZhOWEwNjBmYTljYjE0NzFkOTllN2NlNj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=MEXlZXl1OnYfmkYhT-dQZfLDJh5O7gSSJflwsSHdm9XmxmbaMX-dpB2RevUoJJ5Crn6ZGRpBb3H6yWhnuV8yaHpaDf9H1sJRQhRX56wAxm9Re5WDsFuJR3A3MdS9ipYg%7EuywQ55WoUagR%7Ee8W%7EZziC5oG7wMx8PRheYHJSq%7EMMWlTwsMPXLNCuW%7EGZfPQvZb%7EuoHPlCaYU0o24Iut31cKbCDOrW2wIvy8FY6rwRpygLLNiFmirfc-aWKWVTadnJIGZNL73Fbz8yfatdJcmVgAf9-wud3qnYVxievyq8bdbL37FyxsIlfRXlFl0dXmW5o-a9JX2BeMyR2ALMKoASpqA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 3.165.102.25, 3.165.102.80, 3.165.102.112, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|3.165.102.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28492928 (27M) [application/zip]\n",
            "Saving to: ‘2024swunivchallenge.zip’\n",
            "\n",
            "2024swunivchallenge 100%[===================>]  27.17M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-07-29 11:23:10 (272 MB/s) - ‘2024swunivchallenge.zip’ saved [28492928/28492928]\n",
            "\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m104.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Zero-shot inference\n",
        "\n",
        "- Inference 데이터셋에서 학습되지 않은 CLIP 모델을 활용해서 이미지의 레이블을 예측하는 task를 수행함.\n",
        "- 코드를 실행하면 결과로 `submission.csv` 파일 생성됨. 해당 파일을 Kaggle leaderboard 에 제출하면 됨."
      ],
      "metadata": {
        "id": "MOUd-njrzCJ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qdf2xXO78pch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67be29c9-083c-476a-a1f5-3026a82664b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 351M/351M [00:02<00:00, 130MiB/s]\n",
            "100%|██████████| 254/254 [00:33<00:00,  7.66it/s]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from natsort import natsorted\n",
        "import os, json, open_clip, torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 0. Settings\n",
        "device = 'cuda:0'\n",
        "model_name = 'ViT-B-16'\n",
        "pretrained = 'openai'\n",
        "root = './'\n",
        "dataset_name = 'Scene'\n",
        "\n",
        "# 1. Load CLIP model.\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained)\n",
        "tokenizer = open_clip.get_tokenizer(model_name)\n",
        "\n",
        "# 2. Load test dataset.\n",
        "ds = ImageFolder(os.path.join(root, dataset_name), transform=preprocess)\n",
        "ds.samples = natsorted(ds.samples)\n",
        "dl = DataLoader(ds, shuffle=False, batch_size=32, num_workers=2)\n",
        "\n",
        "# 3. Load class name list.\n",
        "with open(os.path.join(root, 'classes.json'), 'r') as j:\n",
        "     class_names = json.loads(j.read())\n",
        "\n",
        "# 4. Perform zero-shot classification.\n",
        "zero_shot_top1 = 0\n",
        "submission = dict({'id_idx':list(range(8100)), 'label':[]})\n",
        "with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "    text = tokenizer([f\"{class_name}\" for class_name in class_names])\n",
        "    text_features = model.encode_text(text)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    model = model.to(device)\n",
        "    for x, y in tqdm(dl):\n",
        "        x = x.cuda(device)\n",
        "        image_features = model.encode_image(x).to('cpu').float()\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        zero_shot_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "        zero_shot_pred = zero_shot_probs.max(dim=-1)[1].tolist()\n",
        "        submission['label'] += zero_shot_pred\n",
        "\n",
        "# 5. Save prediction as submission.scv file.\n",
        "pd.DataFrame(submission).to_csv(os.path.join(root, 'submission.csv'), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bPSrbjlfwIGa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}